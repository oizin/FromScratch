{"title":"Binary outcome misclassification","markdown":{"yaml":{"title":"Binary outcome misclassification","subtitle":"Nondifferential misclassification","date":"January 2025","bibliography":"references.bib","format":{"html":{"code-fold":"show"}},"execute":{"freeze":"auto","cache":true}},"headingText":"Measurement error","containsRefs":true,"markdown":"\n\n```{r}\n#| echo: false\n#| warning: false\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(patchwork)\n```\n\nMisclassification of binary outcome data can lead to biased estimates of model coefficients when using logistic regression. In certain situations where we have information on the stucture of the misclassification problem there exist methods to estimate bias-corrected coefficients. Such methods can also be used in cases where we are unsure of the exact structure of the misclassification problem and wish to perform sensitivity analyses.\n\n\nIn a misclassified binary outcome setting, rather than observing the true outcome of interest $Y$, we observe $Y^*$. Our goal is to estimate the impact of covariates $X$ on the outcome $Y$ using logistic regression, with particular interest in the covariate coefficients $\\beta_X$:\n\n$$P(Y=1|X=x_i) = \\sigma(\\beta_0 + \\beta_X x_i)$$\n\nWhere $\\sigma(x) = 1/(1+e^{-x})$ is the logistic link function and we assume the covariates $X$ are measured without error. An important consideration is whether the measurement error is nondifferential, where the mismeasured outcome is independent of the covariates given the true outcome:\n\n$$Y^* \\perp X | Y$$\n\nTwo key quantities in bias-correcting our coefficient estimates are the sensitivity and specificity of our measurement process. The sensitivity $\\text{sens} = P(Y^*=1|Y=1)$ or true positive rate quantifies the likelihood positive observations $Y^*=1$ are in fact positive cases $Y=1$. The specificity $\\text{spec} = P(Y^*=0|Y=0)$ or false positive rate quantifies the likelihood negative observations $Y^*=0$ are in fact negative cases $Y=0$.\n\n### Likelihood and estimation\n\nFor a study of size $n$ the likelihood for our observed data is:\n\n$$L(\\beta_0,\\beta_X;Y^*,X) = \\prod_{i=1}^n P(Y^*=y^*_i|X=x_i)=\\prod_{i=1}^n \\sum_{y=0}^1 P(Y=y|X=x_i)P(Y^*=y^*_i|X=x_i,Y=y)$$\n\nIf we assume nondifferential misclassification then the second term in the likelihood can be expressed directly in terms of the sensitivity and specificity of our measurement process:\n\n$$\n\\begin{split}\n    L(\\beta_0,\\beta_X;Y^*,X) &= \\prod_{i=1}^n \\sum_{y=0}^1  P(Y=y|X=x_i)P(Y^*=y^*_i|X=x_i,Y=y) \\\\\n    &= \\prod_{i=1}^n \\sum_{y=0}^1  P(Y=y|X=x_i) P(Y^*=y^*_i|Y=y) \\\\\n    &= \\prod_{i=1}^n P(Y=0|X=x_i) P(Y^*=y^*_i|Y=0) + P(Y=1|X=x_i) P(Y^*=y^*_i|Y=1) \\\\\n    &= \\begin{split}\n        \\prod_{i=1}^n [P(Y=0|X=x_i) P(Y^*=1|Y=0) + P(Y=1|X=x_i) P(Y^*=1|Y=1)]^{y_i} \\times \\\\\n            [P(Y=0|X=x_i) P(Y^*=0|Y=0) + P(Y=1|X=x_i) P(Y^*=0|Y=1)]^{1-y^*_i} \n    \\end{split} \\\\\n    &=  \\begin{split}\n        \\prod_{i=1}^n [P(Y=0|X=x_i) (1-\\text{spec}) + P(Y=1|X=x_i) \\text{sens}]^{y^*_i} \\times \\\\\n            [P(Y=0|X=x_i) \\text{spec} + P(Y=1|X=x_i) (1-\\text{sens})]^{1-y^*_i} \n    \\end{split} \\\\\n\\end{split}\n$$\n\nWhere the second line uses the nondifferential error. For optimisation purposes we often target minimising the negative log-likelihood:\n\n$$\nl(\\beta_0,\\beta_X;Y^*,X) =  \n    \\begin{split}\n        \\sum_{i=1}^n y^*_i \\text{log}[(1-\\sigma(\\beta_0 + \\beta_X x_i)) (1-\\text{spec}) + \\sigma(\\beta_0 + \\beta_X x_i) \\text{sens}] + \\\\\n            (1-y^*_i) \\text{log}[(1-\\sigma(\\beta_0 + \\beta_X x_i)) \\text{spec} + \\sigma(\\beta_0 + \\beta_X x_i) (1-\\text{sens})]\n    \\end{split} \n$$\n\nNotice how there is nothing in this final form of the likelihood that cannot be replaced by the observed data $(Y^*,X)$, or the known (or assumed) sensitivity and specificity values. This can now be directly optimised using general purpose optimisation routines in R (e.g. `stats::optim`), python (e.g. `scipy.optimize`) etc. See 5.4.4 of @shaw2020stratos for a list of papers that developed this methodology.\n\n## Bias corrected model fitting functions\n\nThe code below uses the above results to adjust the logistic regression fitting estimation process to account for the sensitivity and specificity of the measurement process.\n\n```{r}\n#| code-fold: show\nlogistic <- function(X,betas) {\n    1 / (1 + exp(- (X %*% betas)))\n}\n\nnLL_mc <- function(betas,X,y,sens,spec) {\n  n <- nrow(X)\n  pY <- logistic(X,betas)\n  t1 <- -y*log(sens*pY + (1-spec)*(1-pY))\n  t2 <- -(1-y)*log((1-sens)*pY + spec*(1-pY))\n  sum(t1 + t2)  \n}\n\nmissclass_glm <- function(formula,sens,spec,data) {\n\n  # initial fit ignoring missclassification\n  b0 <- coef(glm(formula,data=data,family = binomial()))\n  \n  # fit accounting for sens and spec\n  y <- model.response(model.frame(formula,data))\n  X <- model.matrix(formula,data)\n  res <- optim(b0,nLL_mc,X=X,y=y,sens=sens,spec=spec,hessian=TRUE)\n  coef <- res$par\n  std_err <- sqrt(diag(solve(res$hessian)))\n  z <- coef / std_err\n  p_value <- (1 - pnorm(abs(z))) * 2\n  \n  # output \n  list(unadjusted = b0,\n       adjusted = data.table(var = names(coef),\n                             coef = coef,\n                             std_err = std_err,\n                             p_value = p_value,\n                             lower = coef - 1.96*std_err,\n                             upper = coef + 1.96*std_err))\n}\n```\n\n## Simulation studies\n\nA question for any method is how well does it work? Below we look at a few different scenarios and compare how the coefficients estimated from the bias-adjusted method compare to those estimates naively using standard logistic regression.\n\n### Data generating process\n\nThe simulated data contains five variables, three binary (A1, A2 and A3) and two continuous (X1 and X2), with A1 and X2 being the \"most important\" (the largest coefficients). There are no interactions and the connection between the probabilistic outcome and covariates is a straightforward logistic link.\n\n```{r}\n#| code-fold: show\nfake_data <- function(sens,spec,betas,n=1000) {\n  # data generation\n  ## categorical variables\n  A1 <- rbinom(n,1,0.3)\n  A2 <- rbinom(n,1,0.7)\n  A3 <- rbinom(n,1,0.2)\n  ## continuous variables\n  X1 <- rnorm(n)\n  X2 <- rnorm(n)\n\n  X <- cbind(rep(1,n),A1,A2,A3,X1,X2)\n  pY <- logistic(X,betas)\n  \n  # flip some outcome values\n  pYs <- pY*sens + (1 - pY)*(1-spec)\n  Ys <- fifelse(pYs > runif(n),1,0)\n\n  data.table(Ys = Ys, A1 = A1, A2 = A2, A3 = A3, X1 = X1, X2 = X2)\n}\n```\n\n### Simulation study 1: Vary sensitivity and specificity\n\nBelow we vary the sensitivity and specificity of the outcome measurement process assessing the impact of this on the average bias $B_M=\\frac{1}{M}\\sum_m (\\hat{\\beta}_m-\\beta)$ across $m=1,..,M$ simulations of the unadjusted logistic regression coefficients, and the degree to which the bias-correction estimation process accounts for any bias. We also assess the coverage of the 95% confidence intervals (Wald approximate) $C_M=\\frac{1}{M}\\sum_m I_{\\beta \\in (\\hat{\\beta}_{m,\\text{95% lower}},\\hat{\\beta}_{m,\\text{95% upper}})}$ around the bias-adjusted coefficients.\n\n```{r}\n#| code-fold: true\nM <- 100\nsens <- seq(0.7,1.0,length.out=6)\nspec <- seq(0.7,1.0,length.out=6)\nbetas <- c(-0.3,1.5,0.1,0.2,0.1,-0.7)\nbias_vary_me <- vector(mode=\"list\",length=length(sens)*length(spec))\ncoverage_vary_me <- vector(mode=\"list\",length=length(sens)*length(spec))\n\nij <- 1\nfor (i in 1:length(sens)) {\n    for (j in 1:length(spec)) {\n        bias_orig <- vector(mode=\"list\",length=M)\n        bias_adjust <- vector(mode=\"list\",length=M)\n        coverage_adjust <- vector(mode=\"list\",length=M)\n        for (m in 1:M) {\n            dt <- fake_data(betas = betas, sens = sens[i], spec = spec[j])\n            models <- missclass_glm(Ys ~ A1 + A2 + A3 + X1 + X2,sens[i],spec = spec[j],dt)\n            bias_orig[[m]] <- models$unadjusted - betas\n            bias_adjust[[m]] <- models$adjusted$coef - betas\n            names(bias_adjust[[m]]) <- models$adjusted$var\n            coverage_adjust[[m]] <- fifelse(models$adjusted$lower <= betas & \n                                            models$adjusted$upper >= betas,1,0)\n        }\n        bias_orig <- as.data.table(do.call(rbind,bias_orig))\n        bias_adjust <- as.data.table(do.call(rbind,bias_adjust))\n        bias_orig[,method := \"unadjusted\"]\n        bias_adjust[,method := \"adjusted\"]\n        bias_ <- rbind(bias_orig,bias_adjust)\n        bias_[,sens := sens[i]]\n        bias_[,spec := spec[j]]\n        bias_vary_me[[ij]] <- bias_\n\n        coverage_ <- as.data.table(do.call(rbind,coverage_adjust))\n        names(coverage_) <- models$adjusted$var\n        coverage_ <- coverage_[,lapply(.SD,mean)]\n        coverage_[,sens := sens[i]]\n        coverage_[,spec := spec[j]]\n        coverage_vary_me[[ij]] <- coverage_\n\n        ij <- ij + 1\n    }\n}\nbias_vary_me <- rbindlist(bias_vary_me)\n```\n\n#### Coefficient bias\n\nAs shown below the bias-adjusted coefficients perform far better than the unadjusted coefficients in capturing the true values of the data generating process (model coefficients).\n\n```{r,fig.width=14,fig.height=8}\n#| code-fold: true\ncols <- c(\"(Intercept)\",\"A1\",\"A2\",\"A3\",\"X1\",\"X2\")\ntab <- bias_vary_me[,lapply(.SD, mean),by=.(method,sens,spec),.SDcols=cols]\ntab1 <- melt(tab,id.vars  = 1:3)\ntab1[,Method := fifelse(method == \"unadjusted\",\"Unadjusted\",\"Adjusted\")]\nggplot(tab1,aes(x=sens,y=value,col=spec,group=spec)) + \n    geom_hline(yintercept = 0.0,linetype = 2) +\n    geom_line() + \n    geom_point() +\n    facet_wrap(~variable*Method,ncol=6) +\n    labs(x = \"Outcome base rate\",y=\"Average bias\") +\n    scale_color_continuous(name=\"Specificity\") +\n    theme_bw(base_size=16) \n```\n\n#### Coefficient 95% confidence interval coverage\n\nThe coverage of these lare sample confidence intervals appears reasonably close to 95% given the low number of simulation repetitions $M=100$ and sample size $n=1000$.\n\n```{r,fig.width=14,fig.height=6}\n#| code-fold: true\ntab <- rbindlist(coverage_vary_me)\ntab1 <- melt(tab,id.vars  = 7:8)\nggplot(tab1,aes(x=sens,y=value,col=spec,group=spec)) + \n    geom_hline(yintercept = 0.95,linetype = 2) +\n    geom_point() +\n    geom_line() +\n    facet_wrap(~variable) +\n    coord_cartesian(ylim = c(0.8,1.0)) +\n    labs(x = \"\",y=\"Confidence interval coverage\") +\n    scale_color_continuous(name=\"Specificity\") +\n    theme_bw(base_size=16) \n```\n\n### Simulation study 2: Vary outcome base rate\n\nBelow we assess the extent to which the degree of bias varies by the outcome base rate. We see that the degree of bias, and importance of using bias-correction matters more when the $Y=1$ is more likely for the current simulation setup.\n\n```{r}\n#| code-fold: true\nM <- 100\nbetas <- c(NA,1.5,0.1,0.2,0.1,-0.7)\nb0 <- seq(-4,0,length.out = 10)\nres_vary_base <- vector(mode=\"list\",length=length(b0))\nfor (i in 1:length(b0)) {\n    betas[1] <- b0[i]\n    res_orig <- vector(mode=\"list\",length=M)\n    res_adjust <- vector(mode=\"list\",length=M)\n    y_rate <- vector(mode=\"numeric\",length=M)\n    for (m in 1:M) {\n        dt <- fake_data(betas = betas, sens = 0.9, spec = 1.0)\n        models <- missclass_glm(Ys ~ A1 + A2 + A3 + X1 + X2,0.9,1.0,dt)\n        res_orig[[m]] <- models$unadjusted - betas\n        res_adjust[[m]] <- models$adjusted$coef - betas\n        names(res_adjust[[m]]) <- models$adjusted$var\n        y_rate <- mean(dt$Ys)\n    }\n    res_orig <- as.data.table(do.call(rbind,res_orig))\n    res_adjust <- as.data.table(do.call(rbind,res_adjust))\n    res_orig[,method := \"unadjusted\"]\n    res_adjust[,method := \"adjusted\"]\n    res <- rbind(res_orig,res_adjust)\n    res[,y_rate := mean(y_rate)]\n    res_vary_base[[i]] <- res\n}\nres_vary_base <- rbindlist(res_vary_base)\n```\n\n```{r,fig.width=14,fig.height=7}\n#| code-fold: true\ncols <- c(\"(Intercept)\",\"A1\",\"A2\",\"A3\",\"X1\",\"X2\")\ntab <- res_vary_base[,lapply(.SD, mean),by=.(method,y_rate),.SDcols=cols]\ntab1 <- melt(tab,id.vars  = 1:2)\nggplot(tab1,aes(x=y_rate,y=value,col=method,group=method)) + \n    geom_hline(yintercept = 0.0,linetype = 2) +\n    geom_point() + \n    geom_line() +\n    facet_wrap(~variable) +\n    labs(x = \"Outcome base rate\",y=\"Average bias\") +\n    scale_color_discrete(name=\"Method\",labels=c(\"Adjusted\",\"Unadjusted\")) +\n    theme_bw(base_size=16)\n```\n\n## Conclusion\n\nIn this situation with nondifferential misclassification error and knowledge of the sensitivity and specificity of the measurement process the outlined method works well, demonstrating a reduction in coefficient bias that varies from small to considerable depending on aspects of the data generation process (outcome base rate) and degree of mismeasurement.\n\n## References\n\n::: {#refs}\n:::\n","srcMarkdownNoYaml":"\n\n```{r}\n#| echo: false\n#| warning: false\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(patchwork)\n```\n\nMisclassification of binary outcome data can lead to biased estimates of model coefficients when using logistic regression. In certain situations where we have information on the stucture of the misclassification problem there exist methods to estimate bias-corrected coefficients. Such methods can also be used in cases where we are unsure of the exact structure of the misclassification problem and wish to perform sensitivity analyses.\n\n## Measurement error\n\nIn a misclassified binary outcome setting, rather than observing the true outcome of interest $Y$, we observe $Y^*$. Our goal is to estimate the impact of covariates $X$ on the outcome $Y$ using logistic regression, with particular interest in the covariate coefficients $\\beta_X$:\n\n$$P(Y=1|X=x_i) = \\sigma(\\beta_0 + \\beta_X x_i)$$\n\nWhere $\\sigma(x) = 1/(1+e^{-x})$ is the logistic link function and we assume the covariates $X$ are measured without error. An important consideration is whether the measurement error is nondifferential, where the mismeasured outcome is independent of the covariates given the true outcome:\n\n$$Y^* \\perp X | Y$$\n\nTwo key quantities in bias-correcting our coefficient estimates are the sensitivity and specificity of our measurement process. The sensitivity $\\text{sens} = P(Y^*=1|Y=1)$ or true positive rate quantifies the likelihood positive observations $Y^*=1$ are in fact positive cases $Y=1$. The specificity $\\text{spec} = P(Y^*=0|Y=0)$ or false positive rate quantifies the likelihood negative observations $Y^*=0$ are in fact negative cases $Y=0$.\n\n### Likelihood and estimation\n\nFor a study of size $n$ the likelihood for our observed data is:\n\n$$L(\\beta_0,\\beta_X;Y^*,X) = \\prod_{i=1}^n P(Y^*=y^*_i|X=x_i)=\\prod_{i=1}^n \\sum_{y=0}^1 P(Y=y|X=x_i)P(Y^*=y^*_i|X=x_i,Y=y)$$\n\nIf we assume nondifferential misclassification then the second term in the likelihood can be expressed directly in terms of the sensitivity and specificity of our measurement process:\n\n$$\n\\begin{split}\n    L(\\beta_0,\\beta_X;Y^*,X) &= \\prod_{i=1}^n \\sum_{y=0}^1  P(Y=y|X=x_i)P(Y^*=y^*_i|X=x_i,Y=y) \\\\\n    &= \\prod_{i=1}^n \\sum_{y=0}^1  P(Y=y|X=x_i) P(Y^*=y^*_i|Y=y) \\\\\n    &= \\prod_{i=1}^n P(Y=0|X=x_i) P(Y^*=y^*_i|Y=0) + P(Y=1|X=x_i) P(Y^*=y^*_i|Y=1) \\\\\n    &= \\begin{split}\n        \\prod_{i=1}^n [P(Y=0|X=x_i) P(Y^*=1|Y=0) + P(Y=1|X=x_i) P(Y^*=1|Y=1)]^{y_i} \\times \\\\\n            [P(Y=0|X=x_i) P(Y^*=0|Y=0) + P(Y=1|X=x_i) P(Y^*=0|Y=1)]^{1-y^*_i} \n    \\end{split} \\\\\n    &=  \\begin{split}\n        \\prod_{i=1}^n [P(Y=0|X=x_i) (1-\\text{spec}) + P(Y=1|X=x_i) \\text{sens}]^{y^*_i} \\times \\\\\n            [P(Y=0|X=x_i) \\text{spec} + P(Y=1|X=x_i) (1-\\text{sens})]^{1-y^*_i} \n    \\end{split} \\\\\n\\end{split}\n$$\n\nWhere the second line uses the nondifferential error. For optimisation purposes we often target minimising the negative log-likelihood:\n\n$$\nl(\\beta_0,\\beta_X;Y^*,X) =  \n    \\begin{split}\n        \\sum_{i=1}^n y^*_i \\text{log}[(1-\\sigma(\\beta_0 + \\beta_X x_i)) (1-\\text{spec}) + \\sigma(\\beta_0 + \\beta_X x_i) \\text{sens}] + \\\\\n            (1-y^*_i) \\text{log}[(1-\\sigma(\\beta_0 + \\beta_X x_i)) \\text{spec} + \\sigma(\\beta_0 + \\beta_X x_i) (1-\\text{sens})]\n    \\end{split} \n$$\n\nNotice how there is nothing in this final form of the likelihood that cannot be replaced by the observed data $(Y^*,X)$, or the known (or assumed) sensitivity and specificity values. This can now be directly optimised using general purpose optimisation routines in R (e.g. `stats::optim`), python (e.g. `scipy.optimize`) etc. See 5.4.4 of @shaw2020stratos for a list of papers that developed this methodology.\n\n## Bias corrected model fitting functions\n\nThe code below uses the above results to adjust the logistic regression fitting estimation process to account for the sensitivity and specificity of the measurement process.\n\n```{r}\n#| code-fold: show\nlogistic <- function(X,betas) {\n    1 / (1 + exp(- (X %*% betas)))\n}\n\nnLL_mc <- function(betas,X,y,sens,spec) {\n  n <- nrow(X)\n  pY <- logistic(X,betas)\n  t1 <- -y*log(sens*pY + (1-spec)*(1-pY))\n  t2 <- -(1-y)*log((1-sens)*pY + spec*(1-pY))\n  sum(t1 + t2)  \n}\n\nmissclass_glm <- function(formula,sens,spec,data) {\n\n  # initial fit ignoring missclassification\n  b0 <- coef(glm(formula,data=data,family = binomial()))\n  \n  # fit accounting for sens and spec\n  y <- model.response(model.frame(formula,data))\n  X <- model.matrix(formula,data)\n  res <- optim(b0,nLL_mc,X=X,y=y,sens=sens,spec=spec,hessian=TRUE)\n  coef <- res$par\n  std_err <- sqrt(diag(solve(res$hessian)))\n  z <- coef / std_err\n  p_value <- (1 - pnorm(abs(z))) * 2\n  \n  # output \n  list(unadjusted = b0,\n       adjusted = data.table(var = names(coef),\n                             coef = coef,\n                             std_err = std_err,\n                             p_value = p_value,\n                             lower = coef - 1.96*std_err,\n                             upper = coef + 1.96*std_err))\n}\n```\n\n## Simulation studies\n\nA question for any method is how well does it work? Below we look at a few different scenarios and compare how the coefficients estimated from the bias-adjusted method compare to those estimates naively using standard logistic regression.\n\n### Data generating process\n\nThe simulated data contains five variables, three binary (A1, A2 and A3) and two continuous (X1 and X2), with A1 and X2 being the \"most important\" (the largest coefficients). There are no interactions and the connection between the probabilistic outcome and covariates is a straightforward logistic link.\n\n```{r}\n#| code-fold: show\nfake_data <- function(sens,spec,betas,n=1000) {\n  # data generation\n  ## categorical variables\n  A1 <- rbinom(n,1,0.3)\n  A2 <- rbinom(n,1,0.7)\n  A3 <- rbinom(n,1,0.2)\n  ## continuous variables\n  X1 <- rnorm(n)\n  X2 <- rnorm(n)\n\n  X <- cbind(rep(1,n),A1,A2,A3,X1,X2)\n  pY <- logistic(X,betas)\n  \n  # flip some outcome values\n  pYs <- pY*sens + (1 - pY)*(1-spec)\n  Ys <- fifelse(pYs > runif(n),1,0)\n\n  data.table(Ys = Ys, A1 = A1, A2 = A2, A3 = A3, X1 = X1, X2 = X2)\n}\n```\n\n### Simulation study 1: Vary sensitivity and specificity\n\nBelow we vary the sensitivity and specificity of the outcome measurement process assessing the impact of this on the average bias $B_M=\\frac{1}{M}\\sum_m (\\hat{\\beta}_m-\\beta)$ across $m=1,..,M$ simulations of the unadjusted logistic regression coefficients, and the degree to which the bias-correction estimation process accounts for any bias. We also assess the coverage of the 95% confidence intervals (Wald approximate) $C_M=\\frac{1}{M}\\sum_m I_{\\beta \\in (\\hat{\\beta}_{m,\\text{95% lower}},\\hat{\\beta}_{m,\\text{95% upper}})}$ around the bias-adjusted coefficients.\n\n```{r}\n#| code-fold: true\nM <- 100\nsens <- seq(0.7,1.0,length.out=6)\nspec <- seq(0.7,1.0,length.out=6)\nbetas <- c(-0.3,1.5,0.1,0.2,0.1,-0.7)\nbias_vary_me <- vector(mode=\"list\",length=length(sens)*length(spec))\ncoverage_vary_me <- vector(mode=\"list\",length=length(sens)*length(spec))\n\nij <- 1\nfor (i in 1:length(sens)) {\n    for (j in 1:length(spec)) {\n        bias_orig <- vector(mode=\"list\",length=M)\n        bias_adjust <- vector(mode=\"list\",length=M)\n        coverage_adjust <- vector(mode=\"list\",length=M)\n        for (m in 1:M) {\n            dt <- fake_data(betas = betas, sens = sens[i], spec = spec[j])\n            models <- missclass_glm(Ys ~ A1 + A2 + A3 + X1 + X2,sens[i],spec = spec[j],dt)\n            bias_orig[[m]] <- models$unadjusted - betas\n            bias_adjust[[m]] <- models$adjusted$coef - betas\n            names(bias_adjust[[m]]) <- models$adjusted$var\n            coverage_adjust[[m]] <- fifelse(models$adjusted$lower <= betas & \n                                            models$adjusted$upper >= betas,1,0)\n        }\n        bias_orig <- as.data.table(do.call(rbind,bias_orig))\n        bias_adjust <- as.data.table(do.call(rbind,bias_adjust))\n        bias_orig[,method := \"unadjusted\"]\n        bias_adjust[,method := \"adjusted\"]\n        bias_ <- rbind(bias_orig,bias_adjust)\n        bias_[,sens := sens[i]]\n        bias_[,spec := spec[j]]\n        bias_vary_me[[ij]] <- bias_\n\n        coverage_ <- as.data.table(do.call(rbind,coverage_adjust))\n        names(coverage_) <- models$adjusted$var\n        coverage_ <- coverage_[,lapply(.SD,mean)]\n        coverage_[,sens := sens[i]]\n        coverage_[,spec := spec[j]]\n        coverage_vary_me[[ij]] <- coverage_\n\n        ij <- ij + 1\n    }\n}\nbias_vary_me <- rbindlist(bias_vary_me)\n```\n\n#### Coefficient bias\n\nAs shown below the bias-adjusted coefficients perform far better than the unadjusted coefficients in capturing the true values of the data generating process (model coefficients).\n\n```{r,fig.width=14,fig.height=8}\n#| code-fold: true\ncols <- c(\"(Intercept)\",\"A1\",\"A2\",\"A3\",\"X1\",\"X2\")\ntab <- bias_vary_me[,lapply(.SD, mean),by=.(method,sens,spec),.SDcols=cols]\ntab1 <- melt(tab,id.vars  = 1:3)\ntab1[,Method := fifelse(method == \"unadjusted\",\"Unadjusted\",\"Adjusted\")]\nggplot(tab1,aes(x=sens,y=value,col=spec,group=spec)) + \n    geom_hline(yintercept = 0.0,linetype = 2) +\n    geom_line() + \n    geom_point() +\n    facet_wrap(~variable*Method,ncol=6) +\n    labs(x = \"Outcome base rate\",y=\"Average bias\") +\n    scale_color_continuous(name=\"Specificity\") +\n    theme_bw(base_size=16) \n```\n\n#### Coefficient 95% confidence interval coverage\n\nThe coverage of these lare sample confidence intervals appears reasonably close to 95% given the low number of simulation repetitions $M=100$ and sample size $n=1000$.\n\n```{r,fig.width=14,fig.height=6}\n#| code-fold: true\ntab <- rbindlist(coverage_vary_me)\ntab1 <- melt(tab,id.vars  = 7:8)\nggplot(tab1,aes(x=sens,y=value,col=spec,group=spec)) + \n    geom_hline(yintercept = 0.95,linetype = 2) +\n    geom_point() +\n    geom_line() +\n    facet_wrap(~variable) +\n    coord_cartesian(ylim = c(0.8,1.0)) +\n    labs(x = \"\",y=\"Confidence interval coverage\") +\n    scale_color_continuous(name=\"Specificity\") +\n    theme_bw(base_size=16) \n```\n\n### Simulation study 2: Vary outcome base rate\n\nBelow we assess the extent to which the degree of bias varies by the outcome base rate. We see that the degree of bias, and importance of using bias-correction matters more when the $Y=1$ is more likely for the current simulation setup.\n\n```{r}\n#| code-fold: true\nM <- 100\nbetas <- c(NA,1.5,0.1,0.2,0.1,-0.7)\nb0 <- seq(-4,0,length.out = 10)\nres_vary_base <- vector(mode=\"list\",length=length(b0))\nfor (i in 1:length(b0)) {\n    betas[1] <- b0[i]\n    res_orig <- vector(mode=\"list\",length=M)\n    res_adjust <- vector(mode=\"list\",length=M)\n    y_rate <- vector(mode=\"numeric\",length=M)\n    for (m in 1:M) {\n        dt <- fake_data(betas = betas, sens = 0.9, spec = 1.0)\n        models <- missclass_glm(Ys ~ A1 + A2 + A3 + X1 + X2,0.9,1.0,dt)\n        res_orig[[m]] <- models$unadjusted - betas\n        res_adjust[[m]] <- models$adjusted$coef - betas\n        names(res_adjust[[m]]) <- models$adjusted$var\n        y_rate <- mean(dt$Ys)\n    }\n    res_orig <- as.data.table(do.call(rbind,res_orig))\n    res_adjust <- as.data.table(do.call(rbind,res_adjust))\n    res_orig[,method := \"unadjusted\"]\n    res_adjust[,method := \"adjusted\"]\n    res <- rbind(res_orig,res_adjust)\n    res[,y_rate := mean(y_rate)]\n    res_vary_base[[i]] <- res\n}\nres_vary_base <- rbindlist(res_vary_base)\n```\n\n```{r,fig.width=14,fig.height=7}\n#| code-fold: true\ncols <- c(\"(Intercept)\",\"A1\",\"A2\",\"A3\",\"X1\",\"X2\")\ntab <- res_vary_base[,lapply(.SD, mean),by=.(method,y_rate),.SDcols=cols]\ntab1 <- melt(tab,id.vars  = 1:2)\nggplot(tab1,aes(x=y_rate,y=value,col=method,group=method)) + \n    geom_hline(yintercept = 0.0,linetype = 2) +\n    geom_point() + \n    geom_line() +\n    facet_wrap(~variable) +\n    labs(x = \"Outcome base rate\",y=\"Average bias\") +\n    scale_color_discrete(name=\"Method\",labels=c(\"Adjusted\",\"Unadjusted\")) +\n    theme_bw(base_size=16)\n```\n\n## Conclusion\n\nIn this situation with nondifferential misclassification error and knowledge of the sensitivity and specificity of the measurement process the outlined method works well, demonstrating a reduction in coefficient bias that varies from small to considerable depending on aspects of the data generation process (outcome base rate) and degree of mismeasurement.\n\n## References\n\n::: {#refs}\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","editor":"visual","theme":"cosmo","date-format":"MMMM, YYYY","title-block-banner":true,"title":"Binary outcome misclassification","subtitle":"Nondifferential misclassification","date":"January 2025","bibliography":["references.bib"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}